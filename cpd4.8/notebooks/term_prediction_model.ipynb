{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75749b53",
   "metadata": {},
   "source": [
    "# Create a Watson Machine Learning model for term assignment\n",
    "\n",
    "This notebook demonstrates how a model and scoring function deployed to Watson Machine Learning can be used for automatic term assignment in Metadata Enrichments in IBM Cloud Pak for Data.\n",
    "The classification approach used by this notebook is based on assumptions that simplify some aspects of the classification task\n",
    "to keep the code reasonably small and simple:\n",
    "- ML-based automatic term assignment predicts terms for tables and columns.\n",
    "- Because column metadata tends to be sparse (often just table and column names), term metadata is optionally treated as supplemental training input.\n",
    "- Only textual metadata is considered. It is mapped to sequences of tokens supporting a bag-of-words text classification approach.\n",
    "- The classifier used in this notebook runs in _multi-class_ mode while a _multi-label_ mode might be more appropriate.\n",
    "- This notebook uses a simple approach to scale probabilities such that they can be compared with results from the other term assignment methods.\n",
    "\n",
    "This is a quick overview of the steps in this notebook:\n",
    "1. Define settings and parameters.\n",
    "2. Create a custom library with logic for feature preparation and scoring.\n",
    "3. Extract metadata from a Watson Knowledge Catalog project or catalog for training.\n",
    "4. Train (and test) a model based on a scikit-learn pipeline involving the custom preprocessing library, a vectorizer, and a classifier.\n",
    "5. Deploy the custom library and model to Watson Machine Learning.\n",
    "6. Create and deploy a custom scoring function supporting multiple assignments per data asset.\n",
    "7. Display the settings to enable the metadata enrichment of a project to assign terms based on the deployed ML artifacts.\n",
    "\n",
    "All steps in this notebook must be run in sequence. Steps marked 'optional' are recommended but do not need to be run. A kernel restart requires rerunning the required steps from the top."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "068b6d36",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Published business terms\n",
    "- A catalog or project with data assets that have terms assigned. This serves as input for training. For projects, the notebook considers only assets reviewed with a Metadata Enrichment. For catalogs, it considers all term assignments.\n",
    "- A Watson Machine Learning deployment space.\n",
    "- If the notebook is not running inside IBM Watson Studio: User ID and password of a user with access to the catalog/project and the deployment space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ffb32",
   "metadata": {},
   "source": [
    "## Set up this notebook\n",
    "\n",
    "Enter the required settings.\n",
    "\n",
    "Recommendation: Consider replacing the calls to getpass.getpass() and input() with clear text values if appropriate to avoid having to enter them each time you run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9346e",
   "metadata": {
    "tags": [
     "initial_setup"
    ]
   },
   "outputs": [],
   "source": [
    "cp4d_url = input(\"Enter your Cloud Pak for Data cluster URL (example: https://cpd-wkc.apps.example.com): \")\n",
    "\n",
    "import pkgutil\n",
    "\n",
    "if pkgutil.find_loader('ibm_watson_studio_lib') is not None:\n",
    "    from ibm_watson_studio_lib import access_project_or_space\n",
    "    wslib = access_project_or_space()\n",
    "    wkc_token = wslib.auth.get_current_token()\n",
    "\n",
    "else:\n",
    "    import requests\n",
    "    import getpass\n",
    "    import os\n",
    "\n",
    "    cp4d_username = input(\"Enter your Cloud Pak for Data user ID: \")\n",
    "    cp4d_password = getpass.getpass(\"Enter your Cloud Pak for Data password: \")\n",
    "\n",
    "    def get_authentication_token():\n",
    "        print(\"Authenticating\")\n",
    "        auth_response = requests.get(cp4d_url + \"/v1/preauth/validateAuth\", headers={\"username\": cp4d_username, \"password\": cp4d_password}, verify=False)\n",
    "        auth_response.raise_for_status()\n",
    "        print(\"Authentication successful\")\n",
    "        wkc_token = auth_response.json()['accessToken']\n",
    "        return wkc_token\n",
    "    wkc_token = get_authentication_token()\n",
    "\n",
    "HIGHLIGHT = \"\\033[1;31;43m\" # Color coding to highlight important messages\n",
    "\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "\n",
    "def initialize_wml_client(wkc_token):\n",
    "    api_client_auth = { 'token': wkc_token, 'url': cp4d_url, \"instance_id\": \"openshift\", 'version': '4.5' }   \n",
    "    print(\"Creating WML client\")\n",
    "    wml_client = APIClient(api_client_auth)\n",
    "    return wml_client\n",
    "\n",
    "wml_client = initialize_wml_client(wkc_token)\n",
    "print(f\"Notebook configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf144341",
   "metadata": {},
   "source": [
    "#### Optional: List available deployment spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225c1d9",
   "metadata": {
    "tags": [
     "list_spaces"
    ]
   },
   "outputs": [],
   "source": [
    "wml_client.spaces.list(limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8558e",
   "metadata": {},
   "source": [
    "#### Optional: List available projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee24f45",
   "metadata": {
    "tags": [
     "list_projects"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def list_projects():\n",
    "    projects_response = requests.get(f\"{cp4d_url}/v2/projects\", headers={\"Authorization\": f\"Bearer {wkc_token}\"}, verify=False)\n",
    "    projects_response.raise_for_status()\n",
    "    projects_json = projects_response.json()\n",
    "    if 'resources' in projects_json:\n",
    "        print(\"------------------------------------  -------------------------\")\n",
    "        print(\"ID                                    NAME\")\n",
    "        for project_info in projects_json['resources']:\n",
    "            if 'entity' in project_info and 'name' in project_info['entity'] and 'metadata' in project_info and 'guid' in project_info['metadata']:\n",
    "                print(f\"{project_info['metadata']['guid']}  {project_info['entity']['name']}\")\n",
    "        print(\"------------------------------------  -------------------------\")\n",
    "\n",
    "list_projects()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2bcbd",
   "metadata": {},
   "source": [
    "#### Optional: List available catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0710204",
   "metadata": {
    "tags": [
     "list_catalogs"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def list_catalogs():\n",
    "    catalogs_response = requests.get(f\"{cp4d_url}/v2/catalogs\", headers={\"Authorization\": f\"Bearer {wkc_token}\"}, verify=False)\n",
    "    catalogs_response.raise_for_status()\n",
    "    catalogs_json = catalogs_response.json()\n",
    "    if 'catalogs' in catalogs_json:\n",
    "        print(\"------------------------------------  -------------------------\")\n",
    "        print(\"ID                                    NAME\")\n",
    "        for catalog_info in catalogs_json['catalogs']:\n",
    "            if 'entity' in catalog_info and 'name' in catalog_info['entity'] and 'metadata' in catalog_info and 'guid' in catalog_info['metadata']:\n",
    "                print(f\"{catalog_info['metadata']['guid']}  {catalog_info['entity']['name']}\")\n",
    "        print(\"------------------------------------  -------------------------\")\n",
    "\n",
    "list_catalogs()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032cb18",
   "metadata": {},
   "source": [
    "### Set deployment space ID and ID of training source (catalog or project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993000d",
   "metadata": {
    "tags": [
     "set_space_and_training_source"
    ]
   },
   "outputs": [],
   "source": [
    "# ID of deployment space for artifacts created by this notebook\n",
    "wml_space_id = input(\"Enter the ID of your Watson Machine Learning deployment space: \")\n",
    "wml_client.set.default_space(wml_space_id)\n",
    "\n",
    "# ID of catalog/project that will be used for training\n",
    "training_source = input(\"Source for training: Enter c for catalog or p for project.\")\n",
    "if training_source != \"c\" and training_source != \"p\":\n",
    "    print(f\"{HIGHLIGHT} Source for training must be 'c' or 'p'\")\n",
    "training_source_type = 'catalog' if training_source == \"c\" else 'project'\n",
    "training_source_id = input(f\"Enter the ID of the {training_source_type} to be used for training: \")\n",
    "\n",
    "print(f\"Artifacts will be deployed to space {wml_space_id}.\")\n",
    "print(f\"Training will be based on assigned/rejected terms in {training_source_type} {training_source_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c25c20",
   "metadata": {},
   "source": [
    "### Define ML parameters\n",
    "\n",
    "Define the parameters for training and scoring. They can be adjusted as needed.\n",
    "\n",
    "#### Parameters for training\n",
    "- `metadata_scope`: \n",
    "  Defines the amount of term metadata that is retrieved as input for training. This parameter can have one of the following values:\n",
    "  - `names_of_assigned_terms`:\n",
    "    Only the names of terms that are assigned to assets are considered. The advantage of this option is that all required\n",
    "    metadata is retrieved from Watson Knowledge Catalog with a single request. However, it disregards additional metadata such as categories or term descriptions\n",
    "    that might help to improve the quality of the model. To include this additional meta data, choose one of the other two options.\n",
    "  - `metadata_of_all_terms`: \n",
    "    The metadata of all business terms as relevant for training is considered, irrespective of whether these terms are\n",
    "    assigned to assets or not. This is the option of choice if no terms are assigned or if the number of assigned terms is small.\n",
    "  - `metadata_of_assigned_terms`: \n",
    "    The metadata of terms that are assigned to assets is considered. This is usually a good choice because it enhances the training\n",
    "    data for assigned assets with additional term metadata such as categories and descriptions.\n",
    "- `reviewed_only`: \n",
    "  If set to True, only manual assignments and assignments on tables or columns that have been reviewed are used for training. If set to False, all assignments are used for training. The default setting is True. This option is only valid if a project is used as the training source. \n",
    "\n",
    "#### Parameters for scoring\n",
    "- `max_num_assignments`:\n",
    "  Limits the number of assignments returned by scoring. Due to the multi-class approach, the sum of confidences for the returned assignments is\n",
    "  always 1.0. Thus, a larger value leads to smaller confidences per assignment, which lowers the impact of ML-based term assignment compared to the other methods. \n",
    "- `assignment_threshold`:\n",
    "  Limits the number of assignments returned by scoring to those with a confidence matching or exceeding this value. A higher `max_num_assignments` value\n",
    "  usually demands a lower `assignment_threshold` value. Otherwise, reasonable assignment might be filtered out.\n",
    "\n",
    "#### Parameters for feature selection (training and scoring)\n",
    "- `asset_metadata`: \n",
    "  Defines the asset metadata used for training and scoring as well as the order in which metadata is mapped to features. The order is important because\n",
    "  it can help to preserve important information in the form of n-grams. An example of an important bigram is the combination of a table name and a column name. See the parameter `ngram_range`\n",
    "  under `feature_mapping` for details on how to enable bigrams.\n",
    "  \n",
    "  The parameter value can be one or more of these keys: `schema_name`, `table_name`, `column_name`, `asset_description`.\n",
    "- `term_metadata`:\n",
    "  Serves same purpose for terms as `asset_metadata` for assets. The parameter value can be one or more of these keys: `category`, `term_name`, `term_description`. If the training parameter `metadata_scope` is set to `names_of_assigned_terms`, only [] or [`term_name`] are allowed.\n",
    "\n",
    "#### Parameters for feature mapping\n",
    "These parameters guide the creation of features from asset and/or term metadata. They are used to configure the scikit-learn CountVectorizer. Defaults are used for parameters not explicitly defined here. See the scikit-learn documentation for details. The parameters are typically used to balance quality versus performance.\n",
    "- `ngram_range`: Binary tuple defining the lower and upper bound for n-gram usage.\n",
    "- `min_df`: Absolute or proportional value enabling the vectorizer to disregard tokens that occur in the metadata of a low number of assets/terms.\n",
    "- `max_df`: Absolute or proportional value enabling the vectorizer to disregard tokens that occur in the metadata of many assets/terms.\n",
    "- `max_features`: Limits the number of features created from the input metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8830b6c",
   "metadata": {
    "tags": [
     "set_parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Change values as appropriate (see section above)\n",
    "parameters = {\n",
    "    \"training\": {\n",
    "        \"metadata_scope\": \"metadata_of_assigned_terms\",\n",
    "        \"reviewed_only\": True\n",
    "    },\n",
    "    \"scoring\": {\n",
    "        \"max_num_assignments\": 2,\n",
    "        \"assignment_threshold\": 0.4\n",
    "    },\n",
    "    \"feature_selection\": {\n",
    "        \"term_metadata\":  [\"category\",   \"term_name\",  \"term_description\"],\n",
    "        \"asset_metadata\": [\"table_name\", \"column_name\"]\n",
    "    },\n",
    "    \"feature_mapping\": {\n",
    "        \"ngram_range\": (1, 2),\n",
    "        \"min_df\": 0,\n",
    "        \"max_df\":  1.0,\n",
    "        \"max_features\": 50000\n",
    "    }\n",
    "}\n",
    "\n",
    "def parameter_values_are_valid():\n",
    "    messages = []\n",
    "    metadata_scope = parameters['training']['metadata_scope']\n",
    "    if metadata_scope not in ['names_of_assigned_terms', 'metadata_of_assigned_terms', 'metadata_of_all_terms']:\n",
    "        messages.append(f\"Invalid training parameter value for 'metadata_scope': {parameters['training']['metadata_scope']}\")\n",
    "    if len(parameters['feature_selection']['asset_metadata']) == 0:\n",
    "        messages.append(\"The feature selection parameter 'asset_metadata' must not be empty\")\n",
    "    if  not all( v in ['schema_name', 'table_name', 'column_name', 'asset_description'] for v in parameters['feature_selection']['asset_metadata'] ):\n",
    "        messages.append(f\"Invalid feature selection parameter value in 'asset_metadata': {parameters['feature_selection']['asset_metadata']}\")\n",
    "    term_metadata = parameters['feature_selection']['term_metadata']\n",
    "    if not all(v in ['category', 'term_name', 'term_description'] for v in term_metadata ):\n",
    "        messages.append(f\"Invalid feature selection parameter value in 'term_metadata': {term_metadata}\")\n",
    "    if metadata_scope == 'names_of_assigned_terms' and ( len(term_metadata) > 1 or (len(term_metadata) == 1 and 'term_name' not in term_metadata) ):\n",
    "        messages.append(f\"The feature selection parameter 'term_metadata' must be [] or ['term_name'] if the training parameter 'metadata_scope' is set to 'names_of_assigned_terms'\" )\n",
    "    if parameters['scoring']['assignment_threshold'] < 0 or parameters['scoring']['assignment_threshold'] > 1:\n",
    "        messages.append(\"The scoring parameter assignment_threshold must be in [0..1]\")\n",
    "    if messages:\n",
    "        print(\"Found invalid parameters:\")\n",
    "        for msg in messages:\n",
    "            print(f\"\\t {msg}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "if parameter_values_are_valid():\n",
    "    if training_source_type == 'catalog':\n",
    "        print(\"Setting the training parameter 'reviewed_only' to 'False' since the training source is a catalog.\\n\")\n",
    "        parameters['training']['reviewed_only'] = False\n",
    "    print(\"Parameters defined:\")\n",
    "    print(json.dumps(parameters, indent = 3))\n",
    "else:\n",
    "    print(\"Fix invalid parameter values and rerun this cell.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6e2fbbc",
   "metadata": {},
   "source": [
    "## Create a library with custom conversion logic for preprocessing and scoring\n",
    "\n",
    "In this section, we will create a custom Python library called `preprocessing_and_scoring_support`. We create a library because the code is used within this notebook and is also required for prediction in a Watson Machine Learning deployment.\n",
    "The library will contain code for these two tasks:\n",
    "1. Preprocess the input for model creation and scoring (`MetadataToTextConverter`)\n",
    "2. Postprocess the result of the scoring function (`ProbabilityToConfidenceConverter`)\n",
    "\n",
    "For details on how to use custom functions with the Watson Machine Learning Python SDK, you can check out the example \"Use scikit-learn and custom library to predict temparature with ibm-watson-machine learning\" at: https://github.com/IBM/watson-machine-learning-samples/blob/master/cpd4.5/notebooks/python_sdk/deployments/custom_library/Use%20scikit-learn%20and%20custom%20library%20to%20predict%20temperature.ipynb\n",
    "\n",
    "#### The `MetadataToTextConverter` handles\n",
    "- Specific characteristics of certain metadata (e.g. technical names versus business terms versus (free) text)\n",
    "- Potential technical characteristics (e.g. name \"density\" due to length limitations,  case sensitivity/insensitivity, code page support)\n",
    "- Custom conventions and policies (naming conventions, use of abbreviations, prefixes, languages, etc.)\n",
    "- Negative assignments (rejected terms)\n",
    "\n",
    "#### The `ProbabilityToConfidenceConverter` handles\n",
    "- Mapping of scoring results returned by the ML algorithm (probabilities) to confidences\n",
    "- Limiting the result size based on the scoring parameters (`max_num_assignments` and `assignment_threshold`)\n",
    "\n",
    "For technical reasons, the Python code for these classes is written to a file. On some environments, this might lead to the code not being properly displayed as Python source code. It can be helpful to temporarily comment out the operating-system-level commands to view the content as Python source code.\n",
    "\n",
    "#### Steps:\n",
    "1. Define Python modules.\n",
    "2. Create and install the Python package.\n",
    "3. Optional: Test the functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d5e4830",
   "metadata": {},
   "source": [
    "### The MetadataToTextConverter\n",
    "\n",
    "The `MetadataToTextConverter` performs the preprocessing required to map metadata of assets and/or terms to sequences of tokens that serve as input to the `CountVectorizer`.\n",
    "The `transform` function reads the input list and processes its content according to the types of metadata included (as defined by the `feature_selection` parameters).\n",
    "Each type of metadata is mapped to a conversion function that applies dedicated preprocessing (tokenization, normalization, application of transformations such as\n",
    "stemming, dictionary expansion, stop-word filtering). Metadata of similar type (such as categories and term_names) can be handled by the same converter function.\n",
    "\n",
    "The function `_create_sequence_of_tokens` handles negative assignments by removing those tokens from the token sequence of a term that are obtained by processing the assets for which this term was rejected thereby lowering or removing the association between these tokens and the term.\n",
    "\n",
    "The mentioned conversion functions contains only hints about the processing that might be of interest for a certain metadata type. `_convert_description_to_sequence_of_words`\n",
    "includes a simple example for stop-word processing for illustration purposes. In general, the individual conversion steps are specific to the use case and the metadata at \n",
    "hand. The `_tokenizer` converts the sequence of words resulting from the converters to a sequence of lowercase tokens, splitting complex words according to the commonly\n",
    "used regular expression `'(?u)\\b\\w\\w*\\b'`.\n",
    "\n",
    "The input to the preprocessor is a list of metadata items that take one of two forms:\n",
    "- Input schema for training:\n",
    "  - The unique identifier of a term (`term_id`). Multiple subsequent metadata items can have the same term ID.\n",
    "  - Values as defined by the feature selection parameter `term_metadata`.\n",
    "  - The polarity: \"+\" for an assignment and \"-\" for a negative assignment.\n",
    "  - Values as defined by the feature selection parameter `asset_metadata`.\n",
    "  \n",
    "  If the same term is assigned to multiple assets, the same term ID and term metadata occur multiple times in this list with different instances of the asset metadata.\n",
    "  \n",
    "  If the same term is assigned to multiple assets, the same term ID and term metadata occur multiple times in this list with different instances of the asset metadata.\n",
    "- Input schema for scoring: Values defined by the feature selection parameter `asset_metadata`.\n",
    "\n",
    "Null values represent metadata that is not present.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "term_metadata = ['category', 'term_name', 'term_description'],\n",
    "asset_metadata = ['table_name', 'column_name', 'asset_description']\n",
    "\n",
    "len(term_metadata) = len(term_metadata) = 3. Thus the size of the input schema for training is 7 = 1 (term id) + 3 (term metadata) + 3 (asset metadata). The size of\n",
    "the input schema for scoring is 3 (asset metadata)\n",
    "\n",
    "1. Example training input representing `Term23` that is assigned to `Table15` and to `Column13` of `Table17`. `Term7` is assigned to `Column24` of `Table15` but\n",
    "rejected for `Column13` of `Table17`. All terms have descriptions. The only asset that has a description is `Column24` of `Table15`. All terms belong to the same category `Category1`.\n",
    "\n",
    "```\n",
    "[ [ \"term_id_of_term23\", \"Category1\", \"Term23\", \"Description of term23\", \"+\", \"Table15\", null, null ],\n",
    "  [ \"term_id_of_term23\", \"Category1\", \"Term23\", \"Description of term23\", \"+\", \"Table17\", \"Column13\", null ],\n",
    "  [ \"term_id_of_term7\",  \"Category1\", \"Term7\", \"Description of term7\",   \"+\", \"Table15\", \"Column24\", \"Description of column24\" ],\n",
    "  [ \"term_id_of_term7\",  \"Category1\", \"Term7\", \"Description of term7\",   \"-\", \"Table17\", \"Column13\", null ]\n",
    "]\n",
    "```\n",
    "\n",
    "2. Example scoring input for `Table42` and `Column4` of `Table9`. Because the first entry represents a table, the second field which represents the column name is null. `Table42` has a description but the column `Column4` does not which is why its third field representing the descriptio is null.\n",
    "\n",
    "```\n",
    "[ [ \"Table42\", null, \"Description of table42\" ],\n",
    "  [ \"Table9\", \"Column4\", null ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18beeb5a",
   "metadata": {
    "tags": [
     "metadata_to_text_converter"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#1. Create dictionary structure\n",
    "mkdir -p term_prediction_model/preprocessing_and_scoring_support\n",
    "\n",
    "#2. Create a file term_prediction_model/preprocessing_and_scoring_support/metadata_to_text_converter.py containing the following code\n",
    "echo \"import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MetadataToTextConverter(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, term_metadata_for_training: list, asset_metadata_for_training: list, asset_metadata_for_scoring: list):\n",
    "        self.asset_metadata_for_scoring = asset_metadata_for_scoring\n",
    "        self.metadata_for_training = ['term_id'] + term_metadata_for_training + ['polarity'] + asset_metadata_for_training\n",
    "        self.index_of_polarity_field = len(term_metadata_for_training) + 1\n",
    "        self.train_on_terms_only = len(asset_metadata_for_training) == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_data_artifact_name_to_sequence_of_words(value):\n",
    "        '''\n",
    "        Consider case style (CamelCase, special separators, abbreviations, prefixes, and so on)\n",
    "        '''\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_glossary_artifact_name_to_sequence_of_words(value):\n",
    "        '''\n",
    "        Consider special conventions for term names\n",
    "        '''\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_description_to_sequence_of_words(value):\n",
    "        '''\n",
    "        Consider stop word removal, stemming, abbreviations, and so on.\n",
    "        Simple example of stop word removal.\n",
    "        '''\n",
    "        stopwords = [ 'a', 'an', 'the', 'in', 'of' ]\n",
    "        result_value = ''\n",
    "        word_list = value.split()\n",
    "        for word in word_list:\n",
    "            if word.lower() not in stopwords:\n",
    "                result_value += word + ' '\n",
    "        return result_value\n",
    "\n",
    "    def _convert_to_sequence_of_words(self, metadata_item: dict, metadata_keys: list, start_index = 0, end_index: int = 0):\n",
    "        i = start_index\n",
    "        sequence_of_words = ''\n",
    "        if end_index == 0:\n",
    "            end_index = len(metadata_item)\n",
    "        while i < end_index:\n",
    "            key = metadata_keys[i]\n",
    "            value = metadata_item[i]\n",
    "            if value is not None:\n",
    "                if key == 'column_name' or key == 'schema_name' or key == 'table_name':\n",
    "                    sow = self._convert_data_artifact_name_to_sequence_of_words(value)\n",
    "                elif key == 'term_name' or key == 'category':\n",
    "                    sow = self._convert_glossary_artifact_name_to_sequence_of_words(value)\n",
    "                elif key == 'asset_description' or key == 'term_description':\n",
    "                    sow = self._convert_description_to_sequence_of_words(value)\n",
    "                else:\n",
    "                    sow = value\n",
    "                if sow:\n",
    "                    sequence_of_words += sow + ' '\n",
    "            i += 1\n",
    "        return sequence_of_words\n",
    "\n",
    "    # Move this logic to the respective converter if this tokenization does not apply to all types of metadata values.\n",
    "    @staticmethod\n",
    "    def _tokenize(sequence_of_words: str):\n",
    "        sequence_of_tokens = ''\n",
    "        # Make sure underscores are treated as separators rather than elements of a name.\n",
    "        sequence_of_words = sequence_of_words.replace('_', '-')\n",
    "        tokens = re.findall(r'(?u)\\b\\w\\w*\\b', sequence_of_words)\n",
    "        is_first = True\n",
    "        for token in tokens:\n",
    "            if is_first:\n",
    "                is_first = False\n",
    "            else:\n",
    "                sequence_of_tokens += ' '\n",
    "            sequence_of_tokens += token.lower()\n",
    "        return sequence_of_tokens\n",
    "\n",
    "    def _create_new_training_record_with_term_metadata(self, metadata_item: dict):\n",
    "        training_record = {\n",
    "            'T': self._convert_to_sequence_of_words(metadata_item, metadata_keys=self.metadata_for_training, start_index=1, end_index=self.index_of_polarity_field),\n",
    "            '+': '',\n",
    "            '-': ''\n",
    "            }\n",
    "        return training_record\n",
    "\n",
    "    def _add_asset_metadata_to_training_record(self, polarity: str, metadata_item: dict, training_record: dict):\n",
    "        if polarity is not None:\n",
    "            training_record[polarity] += self._convert_to_sequence_of_words(metadata_item, metadata_keys=self.metadata_for_training, start_index=self.index_of_polarity_field + 1)\n",
    "        return training_record\n",
    "\n",
    "    def _create_sequence_of_tokens(self, training_record: dict):\n",
    "        result_sequence = self._tokenize(training_record['T'])\n",
    "        # Don't add asset metadata if only negative assignments are present\n",
    "        if not self.train_on_terms_only and len(training_record['+']) > 0:\n",
    "            positive_sequence_of_tokens = self._tokenize(training_record['+'])\n",
    "            negative_sequence_of_tokens = self._tokenize(training_record['-'])\n",
    "            positive_list_of_tokens = positive_sequence_of_tokens.split()\n",
    "            negative_list_of_tokens = negative_sequence_of_tokens.split()\n",
    "            # Remove tokens from negative assignments from the list of tokens from (positive) assignments\n",
    "            for token in positive_list_of_tokens:\n",
    "                if token not in negative_list_of_tokens:\n",
    "                    if len(result_sequence) > 0:\n",
    "                        result_sequence += ' '\n",
    "                    result_sequence += token\n",
    "        return result_sequence\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Optionally apply preparatory processing in support of the transform function.\n",
    "        return self\n",
    "\n",
    "    def transform(self, metadata_items, y=None):\n",
    "        '''\n",
    "        This function converts a list of metadata items to a list holding sequences of tokens representing terms. It calls the\n",
    "        appropriate conversion function per metadata item depending on its metadata type, applies tokenization on the result, and\n",
    "        creates a list with one entry (sequence of tokens separated with blank) per term that serves as input for the CountVectorizer.\n",
    "        \n",
    "        Example (training):\n",
    "        [ [ term_id1, cat1, term1, tdesc1, \"+\", tab3, tab3col1, tab3col1desc ],\n",
    "          [ term_id1, cat1, term1, tdesc1, \"-\", tab5, tab5col2, tab5col2desc ],\n",
    "          [ term_id2, cat2, term2, tdesc2, \"+\", tab7, tab7col1, tab7col1desc ]\n",
    "        ]\n",
    "        \n",
    "        becomes:\n",
    "        [ 'ts(cat1) ts(term1) ts(tdesc1) ts(tab3) - (ts(tab5) + ts(tab5col2) + ts(tab5col2desc)) + ts(tab3col1) - (ts(tab5) + ts(tab5col2) + ts(tab5col2desc)) + ts(tab7) - (ts(tab5) + ts(tab5col2) + ts(tab5col2desc)),\n",
    "          'ts(term2_category) ts(term2_name) ts(term2_description) ts(asset7_table_name) ts(asset7_column_name) ts(asset7_decription)'\n",
    "        ]\n",
    "        where\n",
    "        ts(mi) returns a sequence of tokens for metadata item mi\n",
    "        ts(mi1) + ts(mi2) concatenates two lists of tokens\n",
    "        ts(mi1) - ts(mi2) removes all occurrences of tokens from ts(mi1) that occur in ts(mi2)\n",
    "\n",
    "        The training_data for a term_id is stored in a 'training record' which is a dictionary of the form:\n",
    "        {\n",
    "            'T': <sequence of words representing term>,\n",
    "            '+': <sequence of words representing all assets to which this term is assigned>,\n",
    "            '-': <sequence of words representing all assets for which this term is rejected>\n",
    "        }\n",
    "        '''\n",
    "        list_of_token_sequences = []\n",
    "        term_id = None\n",
    "        training_record = None\n",
    "        for metadata_item in metadata_items:\n",
    "            # The input is for scoring if the metadata_item consists of only asset metadata for scoring\n",
    "            is_scoring_input = len(metadata_item) == len(self.asset_metadata_for_scoring)\n",
    "            if is_scoring_input:\n",
    "                sequence_of_words = self._convert_to_sequence_of_words(metadata_item, metadata_keys=self.asset_metadata_for_scoring)\n",
    "                list_of_token_sequences.append(self._tokenize(sequence_of_words))\n",
    "            else:\n",
    "                polarity = None if len(metadata_item) <= self.index_of_polarity_field else metadata_item[self.index_of_polarity_field]\n",
    "                if metadata_item[0] == term_id:\n",
    "                    # current metadata_item is training data for same term\n",
    "                    training_record = self._add_asset_metadata_to_training_record(polarity, metadata_item, training_record)\n",
    "                else:\n",
    "                    if training_record is not None:\n",
    "                        # Switch to a different term: Create and store sequence of tokens for previous term and start a new entry\n",
    "                        sequence_of_tokens = self._create_sequence_of_tokens(training_record)\n",
    "                        if len(sequence_of_tokens) > 0:\n",
    "                            list_of_token_sequences.append(sequence_of_tokens)\n",
    "                    term_id = metadata_item[0]\n",
    "                    training_record = self._create_new_training_record_with_term_metadata(metadata_item)\n",
    "                    training_record = self._add_asset_metadata_to_training_record(polarity, metadata_item, training_record)\n",
    "        if training_record:\n",
    "            sequence_of_tokens = self._create_sequence_of_tokens(training_record)\n",
    "            if len(sequence_of_tokens) > 0:\n",
    "                list_of_token_sequences.append(sequence_of_tokens)\n",
    "        return list_of_token_sequences\n",
    "\" > term_prediction_model/preprocessing_and_scoring_support/metadata_to_text_converter.py\n",
    "    \n",
    "echo \"Created metadata_to_text_converter.py.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b80ade",
   "metadata": {},
   "source": [
    "### The ProbabilityToConfidenceConverter\n",
    "\n",
    "The `ProbabilityToConfidenceConverter` is used by the custom scoring function defined later in this notebook. In this sample implementation, it filters\n",
    "the probabilities returned by the ML algorithm's scoring function to a subset matching the filter criteria of the scoring parameters `max_num_assignments`\n",
    "and `assignment_threshold`.\n",
    "\n",
    "This serves two purposes:\n",
    " 1. It reduces the size of the result by considering only the assignments with the highest probabilities.\n",
    " 2. It scales the now smaller number of results by adding the sum of the remaining probabilities (filtered by the scoring parameters) proportionally to the remaining\n",
    "    probabilities. The sum of these remaining probabilities still rounds to 1 though not all of them may show up on the result due to the `assignment_threshold`.\n",
    "\n",
    "This is done only for the purpose of illustration because the sum of confidences for all assignments does not need to be 1. Thus, the confidences returned\n",
    "by the scoring function cannot be easily compared to confidences returned by the other term assignment methods. To keep the example simple, this notebook\n",
    "uses a Naive Bayes classifier that supports multi-class classification with probabilities. A more natural (and theoretically more sound) notion of a confidence value\n",
    "would require a multi-label-classification approach such as one-vs-rest. However, this would make the example significantly more complex, and performance\n",
    "can be a challenge when dealing with a large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afa796",
   "metadata": {
    "tags": [
     "probability_to_confidence_converter"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 3. Create a file term_prediction_model/preprocessing_and_scoring_support/probability_to_confidence_converter.py containing the code in this cell\n",
    "\n",
    "echo \"import numpy as np\n",
    "\n",
    "class ProbabilityToConfidenceConverter:\n",
    "\n",
    "    def __init__(self, max_num_assignments: int, assignment_threshold: float):\n",
    "        self.max_num_assignments = max_num_assignments\n",
    "        self.assignment_threshold = assignment_threshold\n",
    "\n",
    "    def compute_confidences(self, probabilities, classes):\n",
    "\n",
    "        # Sort probabilities and classes (= term_ids) in decending probability order\n",
    "        probabilities, matching_term_ids = (list(t) for t in zip(*sorted(zip(probabilities, classes), reverse=True, key=lambda x: x[0])))\n",
    "\n",
    "        max_num_assignments = self.max_num_assignments\n",
    "        assignment_threshold = self.assignment_threshold\n",
    "\n",
    "        # Sum of probabilities for disregarded results (beyond max_num_assignments)\n",
    "        sum_disregarded_probabilities = np.sum(probabilities[max_num_assignments:])\n",
    "        # Keep results only up to max_num_assignments\n",
    "        matching_probabilities = probabilities[:max_num_assignments]\n",
    "        matching_term_ids = matching_term_ids[:max_num_assignments]\n",
    "        sum_matching_probabilities = np.sum(matching_probabilities)\n",
    "        if sum_matching_probabilities == 0:\n",
    "            raise RuntimeError('The sum of matching probabilities may not be zero.')\n",
    "        \n",
    "        # The adjustment factor is the proportion of disregarded probabilies versus matching probabilities\n",
    "        adjustment_factor = sum_disregarded_probabilities / sum_matching_probabilities\n",
    "\n",
    "        # Multiplying the adjustment factor with the matching probabilities gives the value to be added to each matching probability such that\n",
    "        # the proportion of the value to be added is the same as the proportions of the matching probabilities.\n",
    "        adjustments = np.multiply(matching_probabilities, adjustment_factor)\n",
    "\n",
    "        # Adding the adjustment to the matching probabilities gives values with the same proportion that sum up to 1.\n",
    "        matching_confidences = np.add(matching_probabilities, adjustments)\n",
    "        confidences = []\n",
    "        term_ids = []\n",
    "        for confidence, term_id in zip(matching_confidences, matching_term_ids):\n",
    "            if confidence >= assignment_threshold:\n",
    "                confidences.append(confidence)\n",
    "                term_ids.append(term_id)\n",
    "        return confidences, term_ids\n",
    "\" > term_prediction_model/preprocessing_and_scoring_support/probability_to_confidence_converter.py\n",
    "\n",
    "echo \"Created probability_to_confidence_converter.py.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e73cf",
   "metadata": {},
   "source": [
    "### Create and install Python package `preprocessing_and_scoring_support`\n",
    "\n",
    "The following commands create a Python package `preprocessing_and_scoring_support` and install it on the current Python environment.\n",
    "\n",
    "Note: If you made changes to the custom functions, the Jupyter kernel might need to be restarted to pick up the changed package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aca55d",
   "metadata": {
    "tags": [
     "create_support_library"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo 'Library containing classes in support of term prediction.' > term_prediction_model/README.md\n",
    "echo '__version__ = \"0.1\"'  > term_prediction_model/preprocessing_and_scoring_support/__init__.py\n",
    "\n",
    "echo \"from setuptools import setup\n",
    "VERSION='0.1'\n",
    "setup(name='preprocessing_and_scoring_support',\n",
    "      version=VERSION,\n",
    "      url='...',\n",
    "      author='...',\n",
    "      author_email='...',\n",
    "      license='...',\n",
    "      packages=[\n",
    "            'preprocessing_and_scoring_support'\n",
    "      ],\n",
    "      zip_safe=False\n",
    ")\" > term_prediction_model/setup.py\n",
    "\n",
    "cd term_prediction_model/\n",
    "python setup.py sdist --formats=zip\n",
    "cd ..\n",
    "mv term_prediction_model/dist/preprocessing_and_scoring_support-0.1.zip .\n",
    "\n",
    "pip3 install --force-reinstall preprocessing_and_scoring_support-0.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee026220",
   "metadata": {},
   "source": [
    "### Optional: Test the MetadataToTextConverter and ProbabilityToConfidenceConverter\n",
    "\n",
    "This is a simple unit test that imports the two custom Python modules from the `preprocessing_and_scoring_support` package and verifies its `transformation` and `compute_confidences` functions based on some basic input data. It can be extended to cover additional cases such as different metadata sets but it's kept simple for illustration purposes.\n",
    "\n",
    "Tip: If you want to step through the custom modules using the debugger you will need to make a couple of modifications:\n",
    "- Comment out the bash commands in the custom modules (`#%%bash`, `#echo \"import...` and copy the import statement following the echo command to the next line).\n",
    "- Comment out the line that writes the result to a file near the end of the cell (`#\" > term_prediction_model/...`).\n",
    "- Replace the environment variables `${WML_TAS_CPD_...}` in the `ProbabilityToConfidenceConverter` with dummy values of the respective type (for example, `${TAS_WML_CP4D_TOKEN_LIFETIME}` with 0 and `${TAS_WML_CP4D_URL}`, `${TAS_WML_CP4D_USER}`, `${TAS_WML_CP4D_PW}` with empty strings).\n",
    "- Comment out the import statements `#from preprocessing_and_scoring_support....` in the cell being debugged such that it refers to the code in the notebook rather than in the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8ebeb",
   "metadata": {
    "tags": [
     "test_support_library"
    ]
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import math\n",
    "\n",
    "from preprocessing_and_scoring_support.metadata_to_text_converter import MetadataToTextConverter\n",
    "from preprocessing_and_scoring_support.probability_to_confidence_converter import ProbabilityToConfidenceConverter\n",
    "\n",
    "class TestMetadataToTextConverter(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.term_metadata = ['category', 'term_name', 'term_description']\n",
    "        self.asset_metadata = ['table_name', 'column_name', 'asset_description']\n",
    "        self.m2tc = MetadataToTextConverter(term_metadata_for_training=self.term_metadata, asset_metadata_for_training=self.asset_metadata, asset_metadata_for_scoring=self.asset_metadata)\n",
    "\n",
    "    def test_training_transformation_term_metadata_only(self):\n",
    "        m2tc_2 = MetadataToTextConverter(term_metadata_for_training=self.term_metadata, asset_metadata_for_training=[], asset_metadata_for_scoring=self.asset_metadata)\n",
    "        #        term_id      category            term_name          term_description\n",
    "        row1 = [ \"term_1_id\", \"Banking Business\", \"Bank Account\",    \"Details of a bank account.\"           ]\n",
    "        row2 = [ \"term_2_id\", \"Banking Business\", \"Bank Statistics\", \"Key metrics of the banking business.\" ]\n",
    "        transform_output = m2tc_2.transform([row1, row2])\n",
    "        self.assertEqual(2, len(transform_output))\n",
    "        self.assertEqual('banking business bank account details bank account', transform_output[0])\n",
    "        self.assertEqual('banking business bank statistics key metrics banking business', transform_output[1])\n",
    "    \n",
    "    def test_training_transformation_asset_metadata_only(self):\n",
    "        m2tc_2 = MetadataToTextConverter(term_metadata_for_training=[], asset_metadata_for_training=self.asset_metadata, asset_metadata_for_scoring=self.asset_metadata)\n",
    "        row1 = [ \"term_1_id\", \"+\", \"BANK_CUSTOMERS\",  None,   None ]\n",
    "        row2 = [ \"term_1_id\", \"+\", \"BANK_CUSTOMERS\",  \"AGE\",  None ]\n",
    "        transform_output = m2tc_2.transform([row1, row2])\n",
    "        self.assertEqual(1, len(transform_output))\n",
    "        self.assertEqual('bank customers bank customers age', transform_output[0])\n",
    "\n",
    "    def test_training_transformation(self):\n",
    "        row1 = [ \"term_1_id\", \"Banking Business\", \"Bank Account\",    \"Details of a bank account.\",           \"+\",     \"ACCOUNTS\",   None,         None ]\n",
    "        row2 = [ \"term_1_id\", \"Banking Business\", \"Bank Account\",    \"Details of a bank account.\",           \"+\",     \"ACCOUNTS\",   \"ACCOUNT_ID\", None ]\n",
    "        row3 = [ \"term_2_id\", \"Banking Business\", \"Bank Statistics\", \"Key metrics of the banking business.\", \"+\",     None,         None,         None ]\n",
    "        transform_output = self.m2tc.transform([row1, row2, row3])\n",
    "        self.assertEqual(2, len(transform_output))\n",
    "        self.assertEqual('banking business bank account details bank account accounts accounts account id', transform_output[0])\n",
    "        self.assertEqual('banking business bank statistics key metrics banking business', transform_output[1])\n",
    "\n",
    "    def test_training_transformation_with_negative_assignments(self):\n",
    "        row1 = [ \"term_1_id\", \"Other Business\", \"Data\", \"Any data.\",  \"+\", \"DATA1\", None,   \"Data item for any business not related to banking.\" ]\n",
    "        row2 = [ \"term_1_id\", \"Other Business\", \"Data\", \"Any data.\",  \"-\", \"DATA2\", \"COL1\", \"Data item for banking business.\"                    ]\n",
    "        transform_output = self.m2tc.transform([row1, row2])\n",
    "        self.assertEqual(1, len(transform_output))\n",
    "        # Note that neither 'item' nor 'banking' occur in the training data\n",
    "        self.assertEqual('other business data any data data1 any not related to', transform_output[0])\n",
    "\n",
    "    def test_training_transformation_with_negative_assignments_negative_only(self):\n",
    "        m2tc_2 = MetadataToTextConverter(term_metadata_for_training=[], asset_metadata_for_training=self.asset_metadata, asset_metadata_for_scoring=self.asset_metadata)\n",
    "        row1 = [ \"term_1_id\", \"+\", \"BANK_CUSTOMERS\",  \"surviving\",   None ]\n",
    "        row2 = [ \"term_2_id\", \"-\", \"BANK_CUSTOMERS\",  \"AGE\",  None        ]\n",
    "        row3 = [ \"term_2_id\", \"-\", \"BANK_CUSTOMERS\",  \"ADDRESS\",  None        ]\n",
    "        row4 = [ \"term_3_id\", \"-\", \"BANK_CUSTOMERS\",  \"PROFESSION\",  None ]\n",
    "        # The training data will not include entries from term2 or term3 because they only consist of negative assignments\n",
    "        transform_output = m2tc_2.transform([row1, row2, row3, row4])\n",
    "        self.assertEqual(1, len(transform_output))\n",
    "        self.assertEqual('bank customers surviving', transform_output[0])\n",
    "\n",
    "    def test_training_transformation_with_negative_assignments_tokens_removed(self):\n",
    "        m2tc_2 = MetadataToTextConverter(term_metadata_for_training=[], asset_metadata_for_training=self.asset_metadata, asset_metadata_for_scoring=self.asset_metadata)\n",
    "        row1 = [ \"term_1_id\", \"+\", \"BANK_CUSTOMERS\",  None,   None ]\n",
    "        row2 = [ \"term_1_id\", \"-\", \"BANK_CUSTOMERS\",  \"AGE\",  None ]\n",
    "        # The negative assignment removes all tokens for term1 resulting in an empty training set\n",
    "        transform_output = m2tc_2.transform([row1, row2])\n",
    "        self.assertEqual(0, len(transform_output))\n",
    "\n",
    "    def test_scoring_transformation(self):\n",
    "        row1 = [ \"ACCOUNTS\",  None,          None ]\n",
    "        row2 = [ \"ACCOUNTS\",  \"ACCOUNT_ID\",  None ]\n",
    "        transform_output = self.m2tc.transform([row1, row2])\n",
    "        self.assertEqual(2, len(transform_output))\n",
    "        self.assertEqual('accounts', transform_output[0])\n",
    "        self.assertEqual('accounts account id', transform_output[1])\n",
    "    \n",
    "\n",
    "class TestProbabilityToConfidenceConverter(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        max_num_term_assignments = 2\n",
    "        assignment_threshold = 0.4\n",
    "        self.p2cc = ProbabilityToConfidenceConverter(max_num_assignments=max_num_term_assignments, assignment_threshold=assignment_threshold)\n",
    "\n",
    "    def test_probability_to_confidence_conversion(self):\n",
    "        probabilities = [0.1, 0.4, 0.2, 0.3]\n",
    "        classes = probabilities\n",
    "        confidences, term_ids = self.p2cc.compute_confidences(probabilities, classes)\n",
    "        self.assertAlmostEqual(1.0, math.fsum(confidences))\n",
    "        i = 1\n",
    "        is_correct = True\n",
    "        while is_correct and i < len(confidences):\n",
    "            if confidences[i - 1] < confidences[i] or term_ids[i - 1] < term_ids[i]:\n",
    "                is_correct = False\n",
    "            i += 1\n",
    "        self.assertTrue(\"Result not in proper order.\", is_correct)\n",
    "\n",
    "\n",
    "test_result = unittest.main(argv=[''], verbosity=3, exit=False)\n",
    "assert len(test_result.result.failures) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7d90b",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "Model creation involves the following steps:\n",
    "1. Retrieve metadata of data assets, columns, and terms needed for model training from the catalog/project with the ID `training_source_id`.\n",
    "2. Create the input for training in the format understood by the custom preprocessor (MetadataToTextConverter).\n",
    "3. Train a classifier based on this metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05916806",
   "metadata": {},
   "source": [
    "### Retrieve metadata of data assets and columns with assigned or removed terms\n",
    "\n",
    "Store the retrieved metadata in dictionaries `terms`, `assets`, `term_id_to_assigned_asset_ids_map`, and `term_id_to_rejected_asset_ids_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1a5fd",
   "metadata": {
    "tags": [
     "retrieve_assignments"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Metadata of data assets in the training source with assigned or removed terms\n",
    "assets = {}\n",
    "\n",
    "# Names of all assigned or removed terms stored in the metadata of the data assets in the training source\n",
    "terms = {}\n",
    "\n",
    "# The repository_id is used to create a global_id from the artifact_id of a term\n",
    "repository_id = None\n",
    "\n",
    "def to_map(list_of_named_objects, value_key = 'value'):\n",
    "    # Utility function: Create a dictionary {\"a\":\"x\", \"b\":\"y\", ...} from a list [ {'name':\"a\", '<value_key>':\"x\"}, {'name':\"b\", '<value_key>':\"y\"}, ...]\n",
    "    result_map = {}\n",
    "    for named_object in list_of_named_objects:\n",
    "        key = named_object['name']\n",
    "        if value_key in named_object:\n",
    "            result_map[key] = named_object[value_key]\n",
    "    return result_map\n",
    "\n",
    "def retrieve_assignments_and_rejections(ignore_reviewed: bool):\n",
    "    global repository_id\n",
    "    term_id_to_assigned_asset_ids_map = {}\n",
    "    term_id_to_rejected_asset_ids_map = {}\n",
    "    num_tables_with_assigned_terms = 0\n",
    "    num_tables_with_rejected_terms = 0\n",
    "    num_columns_with_assigned_terms = 0\n",
    "    num_columns_with_rejected_terms = 0\n",
    "    term_id = None  # Keep one term_id to retrieve repository_id\n",
    "\n",
    "    def add_term_ids_for_asset_id(asset_id, term_list, term_id_to_asset_ids_map):\n",
    "        # Add asset_id to all entries of term_id_to_asset_ids_map for all term_ids in term_list and add the corresponding term names to the global terms dictionary\n",
    "        for term in term_list:\n",
    "            nonlocal term_id\n",
    "            term_id = term.get('term_id', None)\n",
    "            if term_id:\n",
    "                if term_id in term_id_to_asset_ids_map:\n",
    "                    term_id_to_asset_ids_map[term_id].append(asset_id)\n",
    "                else:\n",
    "                    term_id_to_asset_ids_map[term_id] = [ asset_id ]\n",
    "                terms[term_id] = { 'term_name': term['term_display_name'] }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {wkc_token}\", \"Content-Type\": \"application/json\"}\n",
    "    params =  {f\"{training_source_type}_id\": training_source_id}\n",
    "    payload = json.dumps({\"query\":f\"asset.{training_source_type}_id:{training_source_id}\", \"limit\":200, \"include\": \"entity\"})\n",
    "\n",
    "    while payload:\n",
    "        search_result_response = requests.post(f\"{cp4d_url}/v2/asset_types/data_asset/search\", headers=headers, params=params, data=payload, verify=False)\n",
    "        search_result_response.raise_for_status()\n",
    "        data_assets = search_result_response.json()['results']\n",
    "        for data_asset in data_assets:\n",
    "            if 'entity' not in data_asset or 'metadata' not in data_asset or 'name' not in data_asset['metadata']:\n",
    "                raise RuntimeError(f'Metadata of data asset is malformed: {json.dumps(data_asset)}')\n",
    "            entity = data_asset['entity']\n",
    "            data_asset_enrichment_area_info = entity.get('metadata_enrichment_area_info',{})\n",
    "            data_asset_has_required_review_status = ignore_reviewed or 'review_date' in data_asset_enrichment_area_info\n",
    "            asset_terms = entity.get('asset_terms', {})\n",
    "            assigned_terms = asset_terms.get('list', [])\n",
    "            asset_id = data_asset['metadata']['asset_id']\n",
    "            property_map = to_map(entity.get('data_asset', {}).get('properties', []))\n",
    "            schema_name = property_map.get('schema_name', '')\n",
    "            table_name = data_asset['metadata']['name']\n",
    "            # Ignore data asset if it has no terms assigned even if terms might be rejected\n",
    "            if data_asset_has_required_review_status and len(assigned_terms) > 0:\n",
    "                num_tables_with_assigned_terms += 1\n",
    "                assets[asset_id] = {\n",
    "                    'schema_name': schema_name,\n",
    "                    'column_name': None,         # column_name == None because this is a table\n",
    "                    'asset_description': data_asset['metadata'].get('description', None),\n",
    "                    'table_name':  table_name\n",
    "                }\n",
    "                add_term_ids_for_asset_id(asset_id, assigned_terms, term_id_to_assigned_asset_ids_map)\n",
    "                rejected_terms = asset_terms.get('rejected_terms', [])\n",
    "                if len(rejected_terms) > 0:\n",
    "                    num_tables_with_rejected_terms += 1\n",
    "                    add_term_ids_for_asset_id(asset_id, rejected_terms, term_id_to_rejected_asset_ids_map)\n",
    "            if 'column_info' in entity:\n",
    "                column_review_dates = None if ignore_reviewed else to_map(data_asset_enrichment_area_info.get('columns', []), value_key='review_date')\n",
    "                for column_name, column in entity['column_info'].items():\n",
    "                    column_has_required_review_status = column_review_dates is None or column_name in column_review_dates\n",
    "                    assigned_column_terms = column.get('column_terms', [])\n",
    "                    # Ignore column if it has no terms assigned even if terms might be rejected\n",
    "                    if column_has_required_review_status and len(assigned_column_terms) > 0:\n",
    "                        num_columns_with_assigned_terms += 1\n",
    "                        column_id = asset_id + ':' + column_name\n",
    "                        assets[column_id] = {\n",
    "                            'schema_name': schema_name,\n",
    "                            'column_name': column_name,\n",
    "                            'asset_description': column.get('column_description', ''),\n",
    "                            'table_name':  table_name\n",
    "                        }\n",
    "                        add_term_ids_for_asset_id(column_id, assigned_column_terms, term_id_to_assigned_asset_ids_map)\n",
    "                        rejected_column_terms = column.get('rejected_terms', [])\n",
    "                        if len(rejected_column_terms) > 0:\n",
    "                            num_columns_with_rejected_terms += 1\n",
    "                            add_term_ids_for_asset_id(column_id, rejected_column_terms, term_id_to_rejected_asset_ids_map)\n",
    "        payload = search_result_response.json().get('next', None)\n",
    "        print(f\"Found {str(num_tables_with_assigned_terms)} table(s) and {str(num_columns_with_assigned_terms)} column(s) with assignments.\")\n",
    "        print(f\"Found {str(num_tables_with_rejected_terms)} table(s) and {str(num_columns_with_rejected_terms)} column(s) with assignments and rejected assignments.\")\n",
    "        repository_id = term_id.split(\"_\")[0] if term_id else None\n",
    "    return term_id_to_assigned_asset_ids_map, term_id_to_rejected_asset_ids_map\n",
    "\n",
    "\n",
    "def retrieve_assignments():\n",
    "    ignore_reviewed = not parameters['training']['reviewed_only']\n",
    "    term_id_to_assigned_asset_ids_map, term_id_to_rejected_asset_ids_map = retrieve_assignments_and_rejections(ignore_reviewed=ignore_reviewed)\n",
    "    if len(term_id_to_assigned_asset_ids_map) == 0:\n",
    "        print(f\"No terms are assigned in this {training_source_type}. The model can only be build based on term meta data.\")\n",
    "        print(\"Setting the training parameter 'metadata_scope' to 'metadata_of_all_terms' and the asset meta data for training to [].\")\n",
    "        parameters['training']['metadata_scope'] = 'metadata_of_all_terms'\n",
    "        asset_metadata_for_training = []\n",
    "    else:\n",
    "        asset_metadata_for_training = parameters['feature_selection']['asset_metadata']\n",
    "    return asset_metadata_for_training, term_id_to_assigned_asset_ids_map, term_id_to_rejected_asset_ids_map\n",
    "\n",
    "asset_metadata_for_training, term_id_to_assigned_asset_ids_map, term_id_to_rejected_asset_ids_map = retrieve_assignments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad81b6e",
   "metadata": {},
   "source": [
    "### Extract additional term metadata (if configured)\n",
    "\n",
    "Extract additional metadata based on the setting of the training parameter `metadata_scope`. If the parameter is set to `metadata_of_all_terms` or `metadata_of_assigned_terms`, the value of the `terms` variable is replaced with a more elaborate dictionary that contains category and description metadata for terms. If the parameter is set to `names_of_assigned_terms`, no further API calls are made and the current value of the `terms` variable is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356ef9d",
   "metadata": {
    "tags": [
     "extend_term_metadata"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, StringIO\n",
    "from csv import DictReader\n",
    "\n",
    "# Add artifact types and corresponding process_xxx_metadata functions to extract metadata from other glossary artifact types as needed\n",
    "glossary_artifact_types = [\"category\", \"glossary_term\"]\n",
    "\n",
    "def export_glossary_metadata():\n",
    "    headers = {\"Authorization\": f\"Bearer {wkc_token}\"}\n",
    "    params = {\"artifact_types\": ','.join(glossary_artifact_types)}\n",
    "    search_result_response = requests.get(f\"{cp4d_url}/v3/governance_artifact_types/export\", headers=headers, params=params, verify=False)\n",
    "    search_result_response.raise_for_status()\n",
    "    in_memory_zip = BytesIO(search_result_response.content)\n",
    "    artifact_exports = {}\n",
    "    with ZipFile(in_memory_zip, 'a') as glossary_export:\n",
    "        infolist = glossary_export.infolist()\n",
    "        for info in infolist:\n",
    "            if not info.is_dir() and info.filename:\n",
    "                with glossary_export.open(info) as glossary_export_csv:\n",
    "                    for glossary_artifact_type in glossary_artifact_types:\n",
    "                        if info.filename.startswith(glossary_artifact_type):\n",
    "                            artifact_exports[glossary_artifact_type] = glossary_export_csv.read().decode()\n",
    "    return artifact_exports\n",
    "\n",
    "def process_glossary_metadata(artifact_type, artifact_exports):\n",
    "    term_export_csv = artifact_exports[artifact_type]\n",
    "    with open(StringIO(term_export_csv), 'r') as terms_csv:\n",
    "        for row in DictReader(terms_csv, delimiter=',', quotechar='\"'):\n",
    "            yield row\n",
    "\n",
    "def retrieve_repository_id(term_id):\n",
    "    headers = {\"Authorization\": f\"Bearer {wkc_token}\"}\n",
    "    response = requests.get(f\"{cp4d_url}/v3/glossary_terms/{term_id}/versions\", headers=headers, verify=False)\n",
    "    response.raise_for_status()\n",
    "    repo_id = response.json().get('resources', [{}])[0].get('metadata', {}).get('source_repository_id', None)\n",
    "    if repo_id is None:\n",
    "        raise RuntimeError(f'Result of GET /v3/glossary_terms/<term_id>/versions is malformed: {json.dumps(response.json())}')\n",
    "    return repo_id\n",
    "\n",
    "def process_term_metadata(category_id_to_category_name_map, artifact_exports, all_terms):\n",
    "    global repository_id\n",
    "    # Read term metadata and extend terms dictionary in support of parameters['feature_selection']['term_metadata']\n",
    "    term_export_csv = artifact_exports[\"glossary_term\"]\n",
    "    for row in DictReader(StringIO(term_export_csv), delimiter=',', quotechar='\"'):\n",
    "        artifact_id = row['Artifact ID']\n",
    "        if artifact_id:\n",
    "            if not repository_id:\n",
    "                repository_id = retrieve_repository_id(artifact_id)\n",
    "            global_id = f\"{repository_id}_{artifact_id}\"\n",
    "            if all_terms or global_id in terms:\n",
    "                category_id = row['Category']\n",
    "                terms[global_id] = {\n",
    "                    'term_name': row['Name'],\n",
    "                    'term_description': row.get('Description', ''),\n",
    "                    'category': category_id_to_category_name_map.get(category_id, '')\n",
    "                }\n",
    "\n",
    "def process_category_metadata(artifact_exports):\n",
    "    category_id_to_category_name_map = {}\n",
    "    category_export_csv = artifact_exports[\"category\"]\n",
    "    for row in DictReader(StringIO(category_export_csv), delimiter=',', quotechar='\"'):\n",
    "        category_id_to_category_name_map[row['Artifact ID']] = row['Name']\n",
    "    return category_id_to_category_name_map\n",
    "\n",
    "def extend_term_metadata(artifact_exports, all_terms):\n",
    "    category_id_to_category_name_map = process_category_metadata(artifact_exports)\n",
    "    # Read term metadata and replace category_id with name of category in each entry of the terms dictionary\n",
    "    process_term_metadata(category_id_to_category_name_map, artifact_exports, all_terms=all_terms)\n",
    "\n",
    "def optionally_extend_term_metadata():\n",
    "    retrieval_option = parameters['training']['metadata_scope']\n",
    "    if retrieval_option == \"names_of_assigned_terms\":\n",
    "        print(f\"Using names of the {str(len(terms))} terms that are assigned or rejected for training.\")\n",
    "    else:\n",
    "        artifact_exports = export_glossary_metadata()\n",
    "        if retrieval_option == \"metadata_of_assigned_terms\":\n",
    "            extend_term_metadata(artifact_exports, all_terms=False)\n",
    "            print(f\"Extended metadata of the {str(len(terms))} terms that are assigned or rejected.\")\n",
    "        else:\n",
    "            # Add/extend metadata of all terms\n",
    "            extend_term_metadata(artifact_exports, all_terms=True)\n",
    "            if len(terms) > 0:\n",
    "                print(f\"Added metadata of all {str(len(terms))} terms.\")\n",
    "\n",
    "optionally_extend_term_metadata()\n",
    "if len(terms) == 0:\n",
    "    print(f\"{HIGHLIGHT} No terms are present in the glossary. A model can not be trained. Re-run the notebook when terms have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3df735",
   "metadata": {},
   "source": [
    "### Prepare metadata for training\n",
    "\n",
    "Create the training input from the term and asset metadata stored in `terms`, `term_id_to_assigned_asset_ids_map`, `term_id_to_rejected_asset_ids_map`, and `assets`.\n",
    "\n",
    "Though not required by the `MetadataToTextConverter`, the training input is created in a compact 'canonical format' that includes the term metadata only in the first row for\n",
    "a term ID to save memory footprint:\n",
    "\n",
    "```\n",
    "[ \n",
    "  [ \"term_id_1\", \"Category\", \"Term1\", \"Term1 description.\", \"+\", ...asset metadata... ],\n",
    "  [ \"term_id_1\", null,       null,    null,                 \"+\", ...asset metadata... ],\n",
    "  [ \"term_id_1\", null,       null,    null,                 \"-\", ...asset metadata... ],\n",
    "  [ \"term_id_1\", null,       null,    null,                 \"-\", ...asset metadata... ],\n",
    "  [ \"term_id_2\", \"Category\", \"Term2\", \"Term2 description.\", \"+\", ...asset metadata... ],\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "The `MetadataToTextConverter` requires all entries of a term ID to be stored in adjacent rows. Term metadata is read from the first row with this term ID. Term metadata in other\n",
    "rows with the same term ID are ignored and can thus be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f260f1",
   "metadata": {
    "tags": [
     "create_training_data"
    ]
   },
   "outputs": [],
   "source": [
    "def create_training_data(terms, assets):\n",
    "    train_data = []\n",
    "    train_data_row_with_term_metadata = None\n",
    "\n",
    "    def add_asset_metadata(polarity, asset_ids, train_data_row_with_term_metadata, subsequent_train_data_row):\n",
    "        for asset_id in asset_ids:\n",
    "            asset = assets[asset_id]\n",
    "            term_metadata_row = train_data_row_with_term_metadata if train_data_row_with_term_metadata else subsequent_train_data_row\n",
    "            if train_data_row_with_term_metadata:\n",
    "                train_data_row_with_term_metadata = None\n",
    "            asset_metadata_row = [ polarity ]\n",
    "            for am in asset_metadata_for_training:\n",
    "                asset_metadata_row.append(asset.get(am, ''))\n",
    "            train_data.append(term_metadata_row + asset_metadata_row)\n",
    "\n",
    "    train_term_ids = []\n",
    "    term_metadata = parameters['feature_selection']['term_metadata']\n",
    "    for term_id, term in terms.items():\n",
    "        train_data_row_with_term_metadata = [ term_id ]\n",
    "        subsequent_train_data_row = [ term_id ]\n",
    "        for tm in term_metadata:\n",
    "            train_data_row_with_term_metadata.append(term.get(tm, ''))\n",
    "            subsequent_train_data_row.append(None)\n",
    "        positive_asset_ids = term_id_to_assigned_asset_ids_map.get(term_id, None)\n",
    "        if positive_asset_ids:\n",
    "            add_asset_metadata(\"+\", positive_asset_ids, train_data_row_with_term_metadata, subsequent_train_data_row)\n",
    "            train_data_row_with_term_metadata = None\n",
    "            negative_asset_ids = term_id_to_rejected_asset_ids_map.get(term_id, None)\n",
    "            if negative_asset_ids:\n",
    "                add_asset_metadata(\"-\", negative_asset_ids, train_data_row_with_term_metadata, subsequent_train_data_row)\n",
    "            train_term_ids.append(term_id)\n",
    "        elif term_metadata:\n",
    "            train_data.append(train_data_row_with_term_metadata)\n",
    "            train_term_ids.append(term_id)\n",
    "    return train_data, train_term_ids\n",
    "\n",
    "train_data, train_term_ids = create_training_data(terms, assets)\n",
    "\n",
    "print(\"Training data created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af36d26",
   "metadata": {},
   "source": [
    "### Optional: Review preprocessed training data\n",
    "\n",
    "The next cell lists the first up to 10 associations between a term (represented by name) and the feature string ('bag of words') representing it in the training input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32669843",
   "metadata": {
    "tags": [
     "review_preprocessed_training_data"
    ]
   },
   "outputs": [],
   "source": [
    "def print_preprocessed_training_input(train_data, train_term_ids):\n",
    "    term_metadata_for_training = parameters['feature_selection']['term_metadata']\n",
    "    m2tc = MetadataToTextConverter(term_metadata_for_training=term_metadata_for_training, asset_metadata_for_training=asset_metadata_for_training, asset_metadata_for_scoring=[])\n",
    "    preprocessing_output = m2tc.transform(train_data)\n",
    "\n",
    "    for i in range(min(len(train_term_ids), 10)):\n",
    "        term_id = train_term_ids[i]\n",
    "        print(f\"{terms[term_id]['term_name']}\\t<-\\t{preprocessing_output[i]}\")\n",
    "\n",
    "print(\"Preprocessed training data:\")\n",
    "print_preprocessed_training_input(train_data, train_term_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51ce33",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Create a training pipeline that includes the following components:\n",
    "- The `MetadataToTextConverter` as preprocessor\n",
    "- A `CountVectorizer` configured with the values in the feature mapping parameters as vectorizer\n",
    "- A Naive Bayes classifier `MultinomialNB` supporting multi-class classification and the computation of probabilities as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5d4b1",
   "metadata": {
    "tags": [
     "train_model"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from preprocessing_and_scoring_support.metadata_to_text_converter import MetadataToTextConverter\n",
    "\n",
    "def train(train_data, term_ids):\n",
    "        model_parameters = parameters['feature_mapping']\n",
    "        args =  {\n",
    "                \"analyzer\": \"word\",\n",
    "                \"preprocessor\": None,\n",
    "                \"tokenizer\": None\n",
    "        }\n",
    "        if 'ngram_range' in  model_parameters:\n",
    "                args['ngram_range'] =  model_parameters['ngram_range']\n",
    "        if 'min_df' in  model_parameters:\n",
    "                args['min_df'] =  model_parameters['min_df']\n",
    "        if 'max_df' in model_parameters:\n",
    "                args['max_df'] =  model_parameters['max_df']\n",
    "        if 'max_features' in  model_parameters:\n",
    "                args['max_features'] =  model_parameters['max_features']\n",
    "        term_metadata_for_training = parameters['feature_selection']['term_metadata']\n",
    "        asset_metadata_for_scoring = parameters['feature_selection']['asset_metadata']\n",
    "        preprocessor = MetadataToTextConverter(asset_metadata_for_training=asset_metadata_for_training, term_metadata_for_training=term_metadata_for_training, asset_metadata_for_scoring=asset_metadata_for_scoring)\n",
    "        vectorizer = CountVectorizer(**args)\n",
    "        classifier = MultinomialNB()\n",
    "        pipeline = Pipeline([('preprocess', preprocessor), ('vectorize', vectorizer),  ('classify', classifier)])\n",
    "        model = pipeline.fit(train_data, term_ids)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        num_features = len(feature_names)\n",
    "        print(f\"Number of features: {str(num_features)}\")\n",
    "        print(f\"First up to 10 features: {feature_names[0:min(num_features, 10)]}\")\n",
    "        term_ids = classifier.classes_.tolist()\n",
    "        return pipeline, model, term_ids\n",
    "\n",
    "pipeline, model, term_ids = train(train_data, train_term_ids)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04dc4d3a",
   "metadata": {},
   "source": [
    "### Define test data\n",
    "\n",
    "This data is used by different tests performed in steps that follow. Therefore, it is kept in the global variable `test_data`. To ensure the test provides useful results, the table names (`TAB1`, `TAB2`) and column names (`CLIENT`, `ADDRESS`) might need to be changed to values that are compatible with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1d58d",
   "metadata": {
    "tags": [
     "define_test_data"
    ]
   },
   "outputs": [],
   "source": [
    "test_data = [ [ \"TAB1\", \"CLIENT\" ], [ \"TAB2\", \"ADDRESS\" ] ]\n",
    "\n",
    "scoring_payload = {\"input_data\": [{\"values\": test_data}]}\n",
    "\n",
    "print(\"Test data defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed275d1a",
   "metadata": {},
   "source": [
    "### Optional: Test model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fe6ab",
   "metadata": {
    "tags": [
     "test_model_locally"
    ]
   },
   "outputs": [],
   "source": [
    "from preprocessing_and_scoring_support.probability_to_confidence_converter import ProbabilityToConfidenceConverter\n",
    "\n",
    "max_num_assignments = parameters['scoring']['max_num_assignments']\n",
    "assignment_threshold = parameters['scoring']['assignment_threshold']\n",
    "ptcc = ProbabilityToConfidenceConverter(max_num_assignments=max_num_assignments, assignment_threshold=assignment_threshold)\n",
    "\n",
    "def limit_to_decimals(f: float, num_decimals: int = 4):\n",
    "    return float(f'%.{num_decimals}f' % f)\n",
    "\n",
    "def test_predict(test_data):\n",
    "    predictions = pipeline.predict_proba(test_data)\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        adjusted_confidences, final_term_ids = ptcc.compute_confidences(predictions[i], term_ids)\n",
    "        print(f\"Prediction for {test_data[i]}:\")\n",
    "        for j in range(len(adjusted_confidences)):\n",
    "            final_term_id = final_term_ids[j]\n",
    "            print(f\"\\t{terms[final_term_id]['term_name']} : {limit_to_decimals(adjusted_confidences[j])}\")\n",
    "\n",
    "test_predict(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80e7b06a",
   "metadata": {},
   "source": [
    "## Deploy model artifacts in Watson Machine Learning\n",
    "\n",
    "This notebook stores the model artifacts directly in a deployment space to simplify the development and test cycle. Alternatively, model artifacts can be stored in a project and promoted to a deployment space as a separate step.\n",
    "\n",
    "Steps:\n",
    "1. Define name prefix for all artifacts to be deployed\n",
    "2. Define names of model artifacts.\n",
    "3. Delete existing deployments for this prefix (if present).\n",
    "4. Store the custom library and add it to the software specification for term prediction.\n",
    "5. Store the model in the default deployment space.\n",
    "6. Deploy the model.\n",
    "7. Optional: Test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a773c",
   "metadata": {},
   "source": [
    "### Define name prefix for all artifacts to be deployed\n",
    "\n",
    "All deployed artifacts created by this notebook share a common prefix. Change the prefix if you want to keep artifacts created by a previous run of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409236d2",
   "metadata": {
    "tags": [
     "set_prefix"
    ]
   },
   "outputs": [],
   "source": [
    "prefix = \"demo\"\n",
    "\n",
    "print(f\"Names of deployed artifacts will have '{prefix}' as prefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802df4c",
   "metadata": {
    "tags": [
     "set_model_artifact_names"
    ]
   },
   "outputs": [],
   "source": [
    "# Artifact names\n",
    "package_extension_name = prefix + \"_tp_library\"\n",
    "software_specification_name = prefix + \"_ta_library_spec\"\n",
    "model_name = prefix + \"_tp_model\"\n",
    "\n",
    "# Deployment name\n",
    "model_deployment_name = model_name + \"_deployment\"\n",
    "\n",
    "# Name to be used in URL for built-in scoring\n",
    "model_serving_name = model_name\n",
    "\n",
    "print(\"Model artifact names defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d3932",
   "metadata": {},
   "source": [
    "### Delete deployed model artifacts (if present)\n",
    "\n",
    "This step is optional when running the notebook for the first time with this prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e35fda",
   "metadata": {
    "tags": [
     "delete_deployed_model_artifacts"
    ]
   },
   "outputs": [],
   "source": [
    "def delete_artifacts(name, wml_resource, wml_resource_name):\n",
    "    print(f\"Deleting {wml_resource_name} with name {name}\")\n",
    "    num_artifacts_deleted = 0\n",
    "    while True:\n",
    "        id = wml_resource.get_id_by_name(name)\n",
    "        if id == 'Not Found':\n",
    "            break\n",
    "        wml_resource.delete(id)\n",
    "        num_artifacts_deleted += 1\n",
    "    print(f\"\\t{num_artifacts_deleted} deleted.\")\n",
    "\n",
    "def delete_deployments(serving_name):\n",
    "    print(f\"Deleting deployments with serving name '{serving_name}'\")\n",
    "    num_deployments_deleted = 0\n",
    "    details = wml_client.deployments.get_details(serving_name=serving_name)\n",
    "    resources = details.get('resources', [])\n",
    "    for resource in resources:\n",
    "        id = wml_client.deployments.get_id(resource)\n",
    "        wml_client.deployments.delete(id)\n",
    "        num_deployments_deleted += 1\n",
    "    print(f\"\\t{num_deployments_deleted} deleted.\")\n",
    "\n",
    "def delete_models(name):\n",
    "    print(f\"Deleting models with name '{name}'\")\n",
    "    num_models_deleted = 0\n",
    "    details = wml_client.repository.get_model_details()\n",
    "    resources = details.get('resources', [])\n",
    "    for resource in resources:\n",
    "        if resource.get('metadata', {}).get('name', '') == name:\n",
    "            id = wml_client.repository.get_model_id(resource)\n",
    "            wml_client.repository.delete(id)\n",
    "            num_models_deleted += 1\n",
    "    print(f\"\\t{num_models_deleted} deleted.\")\n",
    "\n",
    "\n",
    "delete_artifacts(package_extension_name, wml_client.package_extensions, \"package extensions\")\n",
    "delete_artifacts(software_specification_name, wml_client.software_specifications, \"software specifications\")\n",
    "delete_deployments(model_serving_name)\n",
    "delete_models(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3ebbbea",
   "metadata": {},
   "source": [
    "### Store the custom library in the deployment space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b9055",
   "metadata": {
    "tags": [
     "store_custom_library"
    ]
   },
   "outputs": [],
   "source": [
    "def store_custom_library():\n",
    "    meta_props_pkg_extn = {\n",
    "        wml_client.package_extensions.ConfigurationMetaNames.NAME: package_extension_name,\n",
    "        wml_client.package_extensions.ConfigurationMetaNames.DESCRIPTION: \"Library supporting term prediction\",\n",
    "        wml_client.package_extensions.ConfigurationMetaNames.TYPE: \"pip_zip\"\n",
    "    }\n",
    "\n",
    "    pkg_extn_details =  wml_client.package_extensions.store(meta_props=meta_props_pkg_extn, file_path=\"preprocessing_and_scoring_support-0.1.zip\")\n",
    "    pkg_extn_id = wml_client.package_extensions.get_id (pkg_extn_details)\n",
    "    return pkg_extn_id\n",
    "\n",
    "package_extension_id = store_custom_library()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc5cc878",
   "metadata": {},
   "source": [
    "## Extend the base software specification with the custom library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022c9a4",
   "metadata": {
    "tags": [
     "extend_base_software_specifiction"
    ]
   },
   "outputs": [],
   "source": [
    "def extend_base_sw_specification(pkg_extn_uid):\n",
    "    \n",
    "    base_software_specification_id = wml_client.software_specifications.get_id_by_name(\"runtime-23.1-py3.10\")\n",
    "\n",
    "    meta_props_sw_spec = {\n",
    "        wml_client.software_specifications.ConfigurationMetaNames.NAME: software_specification_name,\n",
    "        wml_client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Software specification of library supporting term prediction\",\n",
    "        wml_client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\"guid\": base_software_specification_id}\n",
    "    }\n",
    "\n",
    "    sw_spec_details = wml_client.software_specifications.store(meta_props=meta_props_sw_spec)\n",
    "    sw_spec_id = wml_client.software_specifications.get_id(sw_spec_details)\n",
    "\n",
    "    print(\"Extending software specification with library supporting term prediction\")\n",
    "    wml_client.software_specifications.add_package_extension(sw_spec_id, pkg_extn_uid)\n",
    "    return sw_spec_id\n",
    "\n",
    "software_specification_id = extend_base_sw_specification(package_extension_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327a450",
   "metadata": {},
   "source": [
    "### Store and deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e3241",
   "metadata": {
    "tags": [
     "store_and_deploy_model"
    ]
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.wml_client_error import WMLClientError\n",
    "\n",
    "def deploy_model():\n",
    "    \n",
    "    meta_props_model = {\n",
    "        wml_client.repository.ModelMetaNames.NAME: model_name,\n",
    "        wml_client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Software specification of term prediction model\",\n",
    "        wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_specification_id,\n",
    "        wml_client.repository.ModelMetaNames.TYPE: \"scikit-learn_1.1\"\n",
    "    }      \n",
    "\n",
    "    # Store model in WML repository\n",
    "    model_metadata = wml_client.repository.store_model(model=pipeline, meta_props=meta_props_model)\n",
    "    model_id = wml_client.repository.get_model_id(model_metadata)\n",
    "\n",
    "    metadata = {\n",
    "        wml_client.deployments.ConfigurationMetaNames.NAME: model_deployment_name,\n",
    "        wml_client.deployments.ConfigurationMetaNames.DESCRIPTION: \"Deployment of term prediction model\",\n",
    "        wml_client.deployments.ConfigurationMetaNames.ONLINE: { \"parameters\": { \"serving_name\": model_serving_name } }\n",
    "    }\n",
    "\n",
    "    model_deployment_id = None\n",
    "    try:\n",
    "        model_deployment_details = wml_client.deployments.create(model_id, meta_props=metadata)\n",
    "        model_deployment_id = wml_client.deployments.get_id(model_deployment_details)\n",
    "    except WMLClientError as e:\n",
    "        if \"non_unique_serving_name\" in e.error_msg:\n",
    "            print(f\"{HIGHLIGHT} A model with serving name '{model_serving_name}' is already deployed. Delete the existing deployment '{model_deployment_name}' or use a different prefix/name.\")\n",
    "    return model_deployment_id\n",
    "\n",
    "model_deployment_id = deploy_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e034785",
   "metadata": {},
   "source": [
    "### Optional: Test model deployment with built-in scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b34e5c",
   "metadata": {
    "tags": [
     "test_builtin_scoring"
    ]
   },
   "outputs": [],
   "source": [
    "deployment_prediction = wml_client.deployments.score(model_serving_name, scoring_payload)\n",
    "\n",
    "print(json.dumps(deployment_prediction, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea0f91",
   "metadata": {},
   "source": [
    "## Custom scoring function\n",
    "\n",
    "The built-in scoring function returns a single `term_id` per asset or column in the `prediction` field and returns probabilities for all terms in the `probabilities` field. However, the requirements for automatic term assigment are as follows:\n",
    "- Pairs of term IDs and confidence values for each term assigned to an asset must exist.\n",
    "- It must be possible to limit the result size by maximum number of terms assigned and by a confidence threshold.\n",
    "\n",
    "To achieve this, a custom scoring function is defined:\n",
    "- It computes confidences from probabilities based on the compute_confidences() function of the custom ProbabilityToConfidenceConverter class.\n",
    "- It returns term IDs and confidences in two lists with the same size: `term_ids` and `confidences`\n",
    "- It limits the size of the result and scales confidences according to the `scoring` parameters `max_num_assignments` and `assignment_threshold`.\n",
    "\n",
    "Steps:\n",
    "1. Define the custom scoring function.\n",
    "2. Delete deployed scoring function (if present)\n",
    "3. Optional: Test the custom scoring function locally.\n",
    "4. Deploy the custom scoring function.\n",
    "5. Test the deployed scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0c5e7",
   "metadata": {
    "tags": [
     "set_scoring_artifact_names"
    ]
   },
   "outputs": [],
   "source": [
    "scoring_function_name = prefix + \"_tp_scoring\"\n",
    "\n",
    "# Deployment name\n",
    "scoring_function_deployment_name = scoring_function_name + \"_deployment\"\n",
    "\n",
    "# Name to be used in URL for custom scoring\n",
    "scoring_function_serving_name = scoring_function_name\n",
    "\n",
    "print(\"Scoring function names defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73784a7",
   "metadata": {},
   "source": [
    "### Delete the deployed scoring function (if present)\n",
    "\n",
    "This step is optional when running the notebook for the first time with this prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abde3e4",
   "metadata": {
    "tags": [
     "delete_scoring_artifact_names"
    ]
   },
   "outputs": [],
   "source": [
    "def delete_functions(name):\n",
    "    print(f\"Deleting functions with name '{name}'\")\n",
    "    num_functions_deleted = 0\n",
    "    details = wml_client.repository.get_function_details()\n",
    "    resources = details.get('resources', [])\n",
    "    for resource in resources:\n",
    "        if resource.get('metadata', {}).get('name', '') == name:\n",
    "            id = wml_client.repository.get_function_id(resource)\n",
    "            wml_client.repository.delete(id)\n",
    "            num_functions_deleted += 1\n",
    "    print(f\"\\t{num_functions_deleted} deleted.\")\n",
    "\n",
    "delete_deployments(scoring_function_serving_name)\n",
    "delete_functions(scoring_function_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3302b038",
   "metadata": {},
   "source": [
    "## Create the custom scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db935ec0",
   "metadata": {
    "tags": [
     "custom_scoring_function"
    ]
   },
   "outputs": [],
   "source": [
    "max_num_assignments = parameters['scoring']['max_num_assignments']\n",
    "assignment_threshold = parameters['scoring']['assignment_threshold']\n",
    "classes = term_ids\n",
    "\n",
    "def tp_score(wml_url=cp4d_url, model_deployment_id=model_deployment_id, wml_space_id=wml_space_id, classes=classes, max_num_assignments=max_num_assignments, assignment_threshold=assignment_threshold):\n",
    "    from ibm_watson_studio_lib import access_project_or_space\n",
    "    from ibm_watson_machine_learning import APIClient\n",
    "    from preprocessing_and_scoring_support.probability_to_confidence_converter import ProbabilityToConfidenceConverter\n",
    "\n",
    "    def convert_output(model_response):\n",
    "        # add the list of classes at the top level\n",
    "        predictions = model_response['predictions']\n",
    "        prediction = predictions[0]\n",
    "        prediction_values = prediction['values']\n",
    "        new_prediction_values = []\n",
    "\n",
    "        ptcc = ProbabilityToConfidenceConverter(max_num_assignments=max_num_assignments, assignment_threshold=assignment_threshold)\n",
    "\n",
    "        # Iterate over prediction results per asset and replace them with values obtained from the ProbabilityToConfidenceConverter\n",
    "        for prediction_value in prediction_values:\n",
    "            probabilities = prediction_value[1]\n",
    "            predicted_confidences, predicted_term_ids = ptcc.compute_confidences(probabilities, classes)\n",
    "            new_prediction_values.append([predicted_term_ids, predicted_confidences])\n",
    "        return {'predictions': [{'fields': ['term_ids', 'confidences'], 'values': new_prediction_values}]}  \n",
    "\n",
    "    def score(payload):\n",
    "        try:\n",
    "            token = access_project_or_space().auth.get_current_token()\n",
    "            client = APIClient({'instance_id': 'openshift', 'token': token, 'url': wml_url, 'version': '4.5'})\n",
    "            client.set.default_space(wml_space_id)\n",
    "            model_response = client.deployments.score(model_deployment_id, payload)\n",
    "            return convert_output(model_response)\n",
    "        except Exception as e:\n",
    "            return {'error': repr(e)}\n",
    "            \n",
    "    return score\n",
    "\n",
    "print(\"Scoring function defined\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d70e4e2c",
   "metadata": {},
   "source": [
    "### Optional: Test the custom scoring function locally\n",
    "\n",
    "This test can only be run in IBM Watson Studio since ithe custom scoring function depends on the `ibm_watson_studio_lib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ff70a",
   "metadata": {
    "tags": [
     "test_custom_scoring_locally"
    ]
   },
   "outputs": [],
   "source": [
    "if pkgutil.find_loader('ibm_watson_studio_lib') is not None:\n",
    "    import json\n",
    "\n",
    "    test_prediction = tp_score()(scoring_payload)\n",
    "    print(json.dumps(test_prediction, indent=2))\n",
    "else:\n",
    "    print(\"The local test for custom scoring is only available when running this notebook in IBM Watson Studio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445505d",
   "metadata": {},
   "source": [
    "### Store and deploy the custom scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0684f",
   "metadata": {
    "tags": [
     "store_and_deploy_custom_scoring"
    ]
   },
   "outputs": [],
   "source": [
    "def deploy_custom_scoring_function():\n",
    "\n",
    "    meta_props_scoring_function = {\n",
    "        wml_client.repository.FunctionMetaNames.NAME: scoring_function_name,\n",
    "        wml_client.repository.FunctionMetaNames.DESCRIPTION: \"Scoring function for term prediction\",\n",
    "        wml_client.repository.FunctionMetaNames.SOFTWARE_SPEC_UID: software_specification_id\n",
    "    }\n",
    "\n",
    "    # Store function in WML space\n",
    "    scoring_function_details = wml_client.repository.store_function(meta_props=meta_props_scoring_function, function=tp_score)\n",
    "    scoring_function_id = wml_client.repository.get_function_id(scoring_function_details)\n",
    "\n",
    "    # Deploy function\n",
    "    meta_props_scoring_function_deployment = {\n",
    "        wml_client.deployments.ConfigurationMetaNames.NAME: scoring_function_deployment_name,\n",
    "        wml_client.repository.FunctionMetaNames.DESCRIPTION: \"Deployment of scoring function for term prediction\",\n",
    "        wml_client.deployments.ConfigurationMetaNames.ONLINE: { \"parameters\": { \"serving_name\": scoring_function_serving_name } }\n",
    "    }\n",
    "    scoring_function_deployment_id = None\n",
    "    try:\n",
    "        scoring_function_deployment_details = wml_client.deployments.create(scoring_function_id, meta_props=meta_props_scoring_function_deployment)\n",
    "        scoring_function_deployment_id = wml_client.deployments.get_id(scoring_function_deployment_details)\n",
    "    except WMLClientError as e:\n",
    "        if \"non_unique_serving_name\" in e.error_msg:\n",
    "            print(f\"{HIGHLIGHT} A scoring function with serving name '{scoring_function_serving_name}' is already deployed. Delete the existing deployment '{scoring_function_deployment_name}' or use a different prefix/name.\")\n",
    "    return scoring_function_deployment_id\n",
    "\n",
    "scoring_function_deployment_id = deploy_custom_scoring_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf3f78",
   "metadata": {},
   "source": [
    "### Test the deployed scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f576b35",
   "metadata": {
    "tags": [
     "test_custom_scoring_deployment"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "deployment_prediction = wml_client.deployments.score(scoring_function_serving_name, scoring_payload)\n",
    "\n",
    "print(json.dumps(deployment_prediction, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004fc3e",
   "metadata": {},
   "source": [
    "## Enable MDE asset for term prediction\n",
    "\n",
    "To enable an MDE asset for term prediction based on the deployed scoring function...\n",
    "- Open its Default Settings.\n",
    "- Select `Custom service` under `Term assignment methods to use` > `Machine learning`.\n",
    "- Click 'Select service'.\n",
    "- Run the cell below and enter the names of the deployment space and scoring function deployment.\n",
    "- Click `Test deployment` to verify that the scoring function is deployed.\n",
    "- Click `Next`.\n",
    "- Enter the input transformation code and output transformation code.\n",
    "- Review and apply by clicking `Select`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c147ed",
   "metadata": {
    "tags": [
     "settings_for_custom_term_assignment"
    ]
   },
   "outputs": [],
   "source": [
    "def print_values():\n",
    "    wml_space_details = wml_client.spaces.get_details(space_id=wml_space_id)\n",
    "    print(f\"Deployment space: {wml_space_details['entity']['name']}\")\n",
    "    print(f\"Deployment:       {scoring_function_deployment_name}\")\n",
    "\n",
    "    input_transformation_code = \"{\\\"input_data\\\":[{\\\"values\\\":$append([ [$$.metadata.name, \\\"\\\"] ], $$.entity.data_asset.columns.[[$$.metadata.name, name]])}]}\"\n",
    "    output_transformation_code = \"{\\\"term_assignments\\\": predictions[0].values ~> $map(function($x){function($z){$count($z) > 1? $z : [$z]}($x[0] ~> $zip($x[1]) ~> $map(function($y){{\\\"term_id\\\": $y[0], \\\"confidence\\\": $y[1]}})) })}\"\n",
    "\n",
    "    print(f\"Input transformation code:  {input_transformation_code}\")\n",
    "    print(f\"Output transformation code: {output_transformation_code}\")\n",
    "\n",
    "print_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad94c861",
   "metadata": {},
   "source": [
    "### Optional: Remove source code for custom library\n",
    "\n",
    "Delete the source files required to build the custom preprocessing and scoring library from the current directory of your build environment. These files can be re-created any time by rerunning the cells of section 'Create a library with custom conversion logic for preprocessing and scoring'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ef491",
   "metadata": {
    "tags": [
     "remove_source_files"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf term_prediction_model/\n",
    "rm -f preprocessing_and_scoring_support-0.1.zip\n",
    "\n",
    "echo \"Source files for custom library deleted from workspace.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb0b62e7bf1b5178cd0265d0df5f31744105b3648f95cc7e0a348b884f4e5f11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
